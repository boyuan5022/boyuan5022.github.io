{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaCy N-Grams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jyTOAOR0WDH5ZL2NOae29PC_Psex4Taa",
      "authorship_tag": "ABX9TyPvBT29qRIVevfmqqq5zQPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7048cc6e531e458893d69306cc505d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2708a9c078bb4980ac7a0fb10c75430f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97806ab6cae54eb39f7f401043cfd793",
              "IPY_MODEL_5f1d4dbd1bdc49c588b99c0178670da7"
            ]
          }
        },
        "2708a9c078bb4980ac7a0fb10c75430f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97806ab6cae54eb39f7f401043cfd793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac4adfaafdac44df9d20d2f276c7cf53",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 235,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 235,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e930f02f4d6641ffb9065b2069f42360"
          }
        },
        "5f1d4dbd1bdc49c588b99c0178670da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1569dbd921c34f6883de4e25c3cc3069",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 235/235 [03:44&lt;00:00,  1.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_012c23112eb241c2bfbf615c640c1cda"
          }
        },
        "ac4adfaafdac44df9d20d2f276c7cf53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e930f02f4d6641ffb9065b2069f42360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1569dbd921c34f6883de4e25c3cc3069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "012c23112eb241c2bfbf615c640c1cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boyuan5022/boyuan5022.github.io/blob/master/SpaCy_N_Grams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RMUqrxzdw3t"
      },
      "source": [
        "Sentence Splitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kv4LRqlg6Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c50e7b-f9d5-4067-9ac4-a7fc792d9204"
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wnl = WordNetLemmatizer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "###################\n",
        "#sentence_splitter\n",
        "###################\n",
        "#tokenizes a sentence, applies stopwords, and detects n-grams.\n",
        "#input: sentence-string to tokenize \n",
        "#input: stop_words-words to remove from sentence\n",
        "#output: list of strings\n",
        "def sentence_splitter(sentence,stop_words):\n",
        "    stop_words=stop_words_input+['\\'s']\n",
        "    stop_words_with_punctuation=stop_words+[c for c in '\\[\\]\\\"(){}?!.,<>/;:+\\'']\n",
        "    more_stop_words=stop_words+['more', 'lot', 'less', 'few', 'some', 'little','many']\n",
        "\n",
        "    #Algorithm\n",
        "    #1.Run sentence in Spacy to create doc, a spacy model for natural language processing.\n",
        "    #2.Use the build in model functionality to create a list of Entities(a well known object like \"Barrak Obama\") and Chunks(Multiple words that are used to describe an object like \"the black cat\")\n",
        "    #3.Before Lemmatizer and Stopwords, we need to do this additional step. If a chunk implies ownership i.e. \"Barrack Obama's black cat\", we must seperate \"Barrack Obama's\" from \"black cat\".\n",
        "    #3 continued. If a chunk contains a numeric descriptor i.e. \"5 Black Cats\", we must seperate \"5\" from \"black cats\".\n",
        "    #3 continued. The rules above only apply if the chunk is not an entity i.e. \"The 3 Musketeers\" must be kept together.\n",
        "    #4.Apply Lemmatizer to list of words\n",
        "    #5.Remove Stopwords from list of words.\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    ents=[ent.text for ent in doc.ents if \" \" in ent.text]\n",
        "    chunks=[chunk.text for chunk in doc.noun_chunks if \" \" in chunk.text]\n",
        "    \n",
        "    #we are resorting chunks and ents by legnth so that if a bigger chunk contains a smaller one, the big on gets resolved properly\n",
        "    ents.sort(key=len, reverse=True)\n",
        "    chunks.sort(key=len, reverse=True)\n",
        "\n",
        "    #we are at step 3 of the algorithm now and the next part will improve our chunks\n",
        "    #use|to seperate chunk into smaller groups and connect words in smaller groups with \"_\"\n",
        "    for chunk in chunks:\n",
        "        #if one entities owns another, we must seperate the 2\n",
        "        new_chunk=chunk.replace(\"'s\",\"'s|\")\n",
        "        #create groups for entities\n",
        "        for ent in ents: \n",
        "            if ent in chunk: new_chunk=re.sub(r\"\\b\"+ent+r\"\\b\",\"|\"+ent.replace(\" \",\"_\")+\"|\",new_chunk)\n",
        "        #commas in english represent a division of entities\n",
        "        new_chunk=chunk.replace(\",\",\"|\")\n",
        "        #create groups for stopwords and special words like few, more, a lot, a little, some\n",
        "        for word in new_chunk.split():\n",
        "            if word.lower() in more_stop_words:\n",
        "                new_chunk=re.sub(r\"\\b\"+word+r\"\\b\",\"|\"+word+\"|\",new_chunk)\n",
        "        #create groups for numbers\n",
        "        new_chunk=re.sub(r\"(\\d+)\",r\"|\\g<1>|\",new_chunk)\n",
        "        #the words not grouped yet are new grouped and we replace the chunk in the sentence with our new grouped chunk\n",
        "        sentence=sentence.replace(chunk,\" \".join([\" \"+wnl.lemmatize(phrase).strip().replace(\" \",\"_\")+\"|yes \" for phrase in new_chunk.split(\"|\")]))\n",
        "    \n",
        "    #add |yes and clean multi word ents\n",
        "    for ent in ents:\n",
        "        sentence=sentence.replace(ent,\" \"+ent.replace(\" \", \"_\").replace(\"'s\",\"\")+\"|yes \")\n",
        "    \n",
        "    #add |yes to single word ents\n",
        "    for ent in doc.ents:\n",
        "        if \" \" not in ent.text and \"(\" not in ent.text and \")\" not in ent.text:\n",
        "          sentence=re.sub(r\"\\b\"+ent.text+r\"\\b\",\" \"+ent.text+\"|yes \",sentence)\n",
        "    \n",
        "    return [wnl.lemmatize(word.replace(\"_\",\" \")) for word in nltk.word_tokenize(sentence.lower()) if word.replace(\"|yes\",\"\") not in stop_words_with_punctuation and word!=\"|yes\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBV8yvbsd2vB"
      },
      "source": [
        "Count Word in Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0V-vUJNBR3g"
      },
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from string import ascii_lowercase\n",
        "import numpy as np\n",
        "#for non jupyter notebook environment use \"from tqdm import tqdm\" instead\n",
        "from tqdm.notebook import tqdm\n",
        "import csv\n",
        "\n",
        "###################\n",
        "#count_words_in_document\n",
        "###################\n",
        "#applies sentence_splitter to cells in an excel file and counts word occurances\n",
        "#input: input_file_path-path of input file\n",
        "#input: output_file_path-path of output file\n",
        "#output: no output directly returned. Creates or modify file at output_file_path\n",
        "def count_words_in_document(input_file_path,output_file_path,column_letter):\n",
        "    count = Counter()\n",
        "    stop_words_input = list(stopwords.words('english')+[\"if\",\"'d\",\"'m\",\"n't\",\"'ve\",\"'re\",\"this\",'these',\"'ll\"])\n",
        "    stop_words_input = list(sorted(stop_words_input,key=len, reverse=True))\n",
        "    #Converts Excel column letters to strings\n",
        "    LETTERS = {letter: str(index) for index, letter in enumerate(ascii_lowercase, start=1)}\n",
        "\n",
        "    data_table = pd.read_excel(input_file_path)\n",
        "    predsentences = data_table[data_table.columns[int(LETTERS[column_letter])-1]].to_list()\n",
        "    pred_sentences = [\"\" if x is np.nan else \" \".join(str(x).splitlines()) for x in predsentences]\n",
        "\n",
        "    #run function of a concatenated string of 100 sentences to improve run time\n",
        "    for i in tqdm(range(0,len(pred_sentences),100)):\n",
        "        upper_bound = min(i+100,len(pred_sentences))\n",
        "        pred_sentences_chunk = \" . \".join(pred_sentences[i:upper_bound])\n",
        "        count.update(sentence_splitter(pred_sentences_chunk,stop_words_input))\n",
        "\n",
        "    with open(output_file_path, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['word','entity','frequency']\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(fieldnames)\n",
        "        for key, value in count.items():\n",
        "            #optional, remove entries that are only character or numbers\n",
        "            if key==\"|yes\" or key.replace(\"|yes\",\"\").isdigit() or re.match(r'^[_\\W]+$', key.replace(\"|yes\",\"\")):\n",
        "                pass\n",
        "            elif \"|\" in key:\n",
        "                writer.writerow(key.split(\"|\") + [value])\n",
        "            else:\n",
        "                writer.writerow([key,\"no\"] + [value])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SobVIB1Qd-oI"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "tL_M34St1smy",
        "outputId": "0fefc44b-9d6b-4e9a-f99b-6a507119b0a3"
      },
      "source": [
        "from collections import Counter\n",
        "count=Counter()\n",
        "stop_words_input = list(stopwords.words('english')+[\"if\",\"'d\",\"'m\",\"n't\",\"'ve\",\"'re\",\"this\",'these',\"'ll\"])\n",
        "stop_words_input = list(sorted(stop_words_input,key=len, reverse=True))\n",
        "sentence_input1=\"I love this dress. i'd get it in both colors if i could! the cut and fit is beautiful, i'd suggest sizing down if you feel like it's too boxy or lacks shape. the bottom skirt is round enough to where you can twirl ( #1 thing to look for in a dress ;) ) and it's just overall a classic pretty dress. my only complaint is that the overlay cut out seems a little bit delicate and i'm afraid it will be ruined after a few wears but it seems to be holding up fine so far and isn't incredibly delicate like\"\n",
        "sentence_input2=\"The fabric and detailing of this dress is of superior quality, but unfortunately it runs huge-- you definitely need to wear a tank or cami underneath. i am 5'9 145lbs with massive shoulders/smaller bust and i got the xs petite!\"\n",
        "sentence_input3=\"Barack Obama quickly walked his 3 dogs, Nancy, Mary Jane, and Rover last night.\"\n",
        "sentence_input4=\"Barack Obama's dog Nancy was taken out for a walk in front of the white house last night.\"\n",
        "sentence_input5=\"I love how different this dress is in terms of the design. it catches light beautifully. the short, biased hem makes the dress fun and flirty. but, this dress is completely sheer. there is no lining which is very disappointing. i do have a short, nude slip that makes this dress work for me. you will need a slip, for sure! unless you are rhianna. the sizing runs a little large, i probably could size down given the loose cut but overall the proportions are flattering showing off my legs. i will re\"\n",
        "print(sentence_splitter(sentence_input1,stop_words_input))\n",
        "print(sentence_splitter(sentence_input2,stop_words_input))\n",
        "print(sentence_splitter(sentence_input3,stop_words_input))\n",
        "print(sentence_splitter(sentence_input4,stop_words_input))\n",
        "print(sentence_splitter(sentence_input5,stop_words_input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['love', 'dress|yes', 'get', 'colors|yes', 'could', 'cut|yes', 'fit', 'beautiful', 'suggest', 'sizing', 'feel', 'like', 'boxy', 'lack', 'shape', 'bottom skirt|yes', 'round', 'enough', 'twirl', ' ', '#', '1|yes', 'thing|yes', 'look', 'dress|yes', 'overall', 'classic pretty dress|yes', 'complaint|yes', 'overlay|yes', 'cut', 'seems', 'little', 'bit', 'delicate', 'afraid', 'ruined', 'wears|yes', 'seems', 'holding', 'fine', 'far', 'incredibly', 'delicate', 'like']\n",
            "['fabric|yes', 'detailing', 'dress|yes', 'superior quality|yes', 'unfortunately', 'run', 'huge', '--', 'definitely', 'need', 'wear', 'tank|yes', 'cami', 'underneath', '5|yes', '9|yes', '145|yes', 'lb|yes', 'massive shoulders|yes', 'smaller bust|yes', 'got', 'xs petite|yes']\n",
            "['barack obama|yes', 'quickly', 'walked', '3|yes', 'dogs|yes', 'nancy|yes', 'mary jane|yes', 'rover|yes', 'last night|yes']\n",
            "[\"barack obama's|yes\", 'dog nancy|yes', 'taken', 'walk|yes', 'front', 'the white house|yes', 'last night|yes']\n",
            "['love', 'different', 'dress|yes', 'term', 'design|yes', 'catch', 'light', 'beautifully', 'short|yes', 'biased hem|yes', 'make', 'dress|yes', 'fun', 'flirty', 'dress|yes', 'completely', 'sheer', 'lining|yes', 'disappointing', 'short|yes', 'nude slip|yes', 'make', 'dress|yes', 'work', 'need', 'slip|yes', 'sure', 'unless', 'rhianna', 'sizing|yes', 'run', 'little', 'large', 'probably', 'could', 'size', 'given', 'loose cut|yes', 'overall', 'proportions|yes', 'flattering', 'showing', 'legs|yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxp8zE8ReArM"
      },
      "source": [
        "Full Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "7048cc6e531e458893d69306cc505d73",
            "2708a9c078bb4980ac7a0fb10c75430f",
            "97806ab6cae54eb39f7f401043cfd793",
            "5f1d4dbd1bdc49c588b99c0178670da7",
            "ac4adfaafdac44df9d20d2f276c7cf53",
            "e930f02f4d6641ffb9065b2069f42360",
            "1569dbd921c34f6883de4e25c3cc3069",
            "012c23112eb241c2bfbf615c640c1cda"
          ]
        },
        "id": "vLgNducNT3PC",
        "outputId": "0c633e7b-da1d-458b-c191-dd56d9d85d87"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "count_words_in_document(\"/content/drive/MyDrive/Colab Notebooks/Reviews.xlsx\",\"/content/drive/MyDrive/Colab Notebooks/worddata.csv\",\"g\")\n",
        "files.download(\"/content/drive/MyDrive/Colab Notebooks/worddata.csv\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7048cc6e531e458893d69306cc505d73",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=235.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b032950e-2107-40ee-bdad-a9cd42bc61d0\", \"worddata.csv\", 1006806)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}